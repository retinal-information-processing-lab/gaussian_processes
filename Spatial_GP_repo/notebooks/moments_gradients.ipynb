{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are column and row vectors handled in Matlab versus Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0 (from utils.py)\n",
      " da_matlab:\n",
      " tensor([[ 176.5000,  219.8333,  263.1667,  306.5000],\n",
      "        [-251.2500, -313.0833, -374.9167, -436.7500],\n",
      "        [ 113.2500,  142.0833,  170.9167,  199.7500]]) \n",
      " da_torch.T:\n",
      " tensor([[ 176.5000,  219.8333,  263.1667,  306.5000],\n",
      "        [-251.2500, -313.0833, -374.9167, -436.7500],\n",
      "        [ 113.2500,  142.0833,  170.9167,  199.7500]]) \n",
      "\n",
      " dlambda_m_torch:\n",
      " tensor([13.7500, 19.9167, 26.0833, 32.2500]) \n",
      " dlambda_m_matlab:\n",
      " tensor([13.7500, 19.9167, 26.0833, 32.2500]) \n",
      "\n",
      " dlambda_var_torch:\n",
      " tensor([-2199.1944, -3161.6944, -4291.7500, -5589.3611]) \n",
      " dlambda_var_matlab:\n",
      " tensor([-2199.1944, -3161.6944, -4291.7500, -5589.3611]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import *\n",
    "\n",
    "# Variables initialized in Matlab. \n",
    "# Copying these lines in matlab (making tensors -> arrays and making vectors m -> COLUMN vectors)\n",
    "\n",
    "a_mat = torch.tensor([[1.0,2.0,3.0,4],[5.0,6.0,7.0,8],[9.0,10.0,11.0,12]])\n",
    "\n",
    "Sigma = torch.tensor([[1.0,2.0,3.0,],[5.0,6.0,7.0],[8.0,7.0,6.0,]]) # shape (ntilde, ntilde)\n",
    "dSigma = torch.tensor([[4.0,5.0,6.0],[5.0,6.0,7.0],[6.0,7.0,8.0]]) # shape (ntilde, ntilde)\n",
    "\n",
    "Sigma = Sigma + Sigma.T\n",
    "dSigma = dSigma + dSigma.T\n",
    "\n",
    "m = torch.tensor([1.0,2.0,3.0]) # shape ntilde\n",
    "\n",
    "dki = torch.tensor([[6.0,7.0,8.0,9.0],[2.0,3.0,4.0,5.0],[9.0,8.0,7.0,6]]) # shape ntilde, nt\n",
    "ki =dki # ntilde, nt\n",
    "\n",
    "dkstar = torch.tensor([3,4,5,6])\n",
    "\n",
    "invV = Sigma*3 # ntilde ntilde\n",
    "\n",
    "# These are the variables defined in funtion lFunc of matthews code to calculate gradients with respect to mean and variance of the avg lambda_tilde \n",
    "# They are in the lfunc (loglikelihood) function cause they are used for the gradient of the loglikelihood\n",
    "\n",
    "# In the following 3 lines I just copied the code in matlab and translated\n",
    "da_matlab =  torch.linalg.pinv(Sigma)@( dki - dSigma@a_mat) \n",
    "dlambda_m_matlab = da_matlab.T@m\n",
    "dlambda_var_matlab = dkstar + torch.sum(-dki*a_mat - ki*da_matlab + 2*da_matlab*torch.linalg.solve(invV, a_mat ) , 0 ).T\n",
    "\n",
    "\n",
    "# In torch vectors and matrices are often transposed, starting from a, which in my code corresponds to K@K_tilde_inv like in the notes\n",
    "\n",
    "a = a_mat.T # shape ( nt, ntilde ), its KKtildeinv\n",
    "\n",
    "K_tilde = Sigma\n",
    "dK_tilde = dSigma\n",
    "\n",
    "dK = dki.T\n",
    "K = ki.T\n",
    "\n",
    "dK_vec = dkstar\n",
    "a@dK_tilde\n",
    "\n",
    "K_tilde_inv = torch.linalg.pinv(K_tilde)\n",
    "\n",
    "V = torch.linalg.inv(invV)\n",
    "\n",
    "da_torch = (dK - a@dK_tilde)@K_tilde_inv # TODO check if it can be made more efficient puling dK out of the parenthesis\n",
    "\n",
    "\n",
    "dlambda_m_torch = da_torch@m\n",
    "dlambda_var_torch = dK_vec + torch.einsum( 'ij,ji->i', 2*da_torch, V@a.T) - torch.einsum( 'ij,ij->i', dK,a ) - torch.einsum( 'ij,ij->i', K, da_torch )\n",
    "\n",
    "# If tranlation is ok the following shoudl be equal. Of course da is transposed in between the two codes. The other two are just vectors\n",
    "\n",
    "print( f' da_matlab:\\n {da_matlab} \\n da_torch.T:\\n {da_torch.T} \\n')\n",
    "print( f' dlambda_m_torch:\\n {dlambda_m_torch} \\n dlambda_m_matlab:\\n {dlambda_m_matlab} \\n')\n",
    "print( f' dlambda_var_torch:\\n {dlambda_var_torch} \\n dlambda_var_matlab:\\n {dlambda_var_matlab} \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " logLK_torch:\n",
      " 41.0 \n",
      " Lkhd_matlab:\n",
      " 41.0 \n",
      "\n",
      " dlogLK_matlab:\n",
      " 127749.63888888988 \n",
      " dlogLK_torch:\n",
      " 127749.63888888987 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradients of the Loglikelihood\n",
    "\n",
    "lambda_m = torch.tensor([1.0,2.0,3.0,4.0]) # shape nt\n",
    "\n",
    "# in Matlab the code is\n",
    "\n",
    "# dLtheta = -0.5*dVstar'*f + dmstar'*(r'-f); where r is a row vector and f is a column vector, thats why r is trasposed\n",
    "\n",
    "# In the matlab script the formula is\n",
    "\n",
    "f = torch.tensor([55.0,4.0,22.0,5.0]) # shape nt\n",
    "r = torch.tensor([23.0,47.0,2.0,1.0]) # shape nt\n",
    "Lkhd_matlab = r@lambda_m - torch.sum(f);\n",
    "dlogLK_matlab = -0.5*dlambda_var_matlab.T@f + dlambda_m_matlab.T@(r-f)\n",
    "\n",
    "# while in the torch script the formula is\n",
    "\n",
    "f_mean = f\n",
    "\n",
    "A=1\n",
    "lambda0=0 # there are no f parameters on matlab so I have to put A = 1 and lambda0 = 0\n",
    "logLK_torch     = A*r@lambda_m   + lambda0*torch.sum(r) - torch.sum(f_mean)\n",
    "dlogLK_torch = r@dlambda_m_torch - A*torch.dot(f_mean, dlambda_m_torch) - 0.5*A*A*torch.dot(f_mean, dlambda_var_torch)\n",
    "\n",
    "print( f' logLK_torch:\\n {logLK_torch} \\n Lkhd_matlab:\\n {Lkhd_matlab} \\n')\n",
    "print( f' dlogLK_matlab:\\n {dlogLK_matlab} \\n dlogLK_torch:\\n {dlogLK_torch} \\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C_matlab:\n",
      " tensor([[ 20.2778, -28.0000,  14.0000],\n",
      "        [-28.0000,  38.6806, -19.3472],\n",
      "        [ 14.0000, -19.3472,   9.6806]]) \n",
      " c_torch:\n",
      " tensor([[ 20.2778, -28.0000,  14.0000],\n",
      "        [-28.0000,  38.6806, -19.3472],\n",
      "        [ 14.0000, -19.3472,   9.6806]]) \n",
      " b_matlab:\n",
      " tensor([-1.1667,  1.9167, -0.9167]) \n",
      " b_torch:\n",
      " tensor([-1.1667,  1.9167, -0.9167]) \n",
      "\n",
      " B_matlab:\n",
      " tensor([[-10.3333,  14.8333,  -6.8333],\n",
      "        [-13.0000,  18.5000,  -8.5000],\n",
      "        [-15.6667,  22.1667, -10.1667]]) \n",
      " B_torch:\n",
      " tensor([[-10.3333,  14.8333,  -6.8333],\n",
      "        [-13.0000,  18.5000,  -8.5000],\n",
      "        [-15.6667,  22.1667, -10.1667]]) \n",
      "\n",
      " KLD_matlab:\n",
      " (34.919132834503294+0j) \n",
      " KLD_torch:\n",
      " 34.91913283450331 \n",
      "\n",
      " dKL_matlab:\n",
      " 110.47222222222383 \n",
      " dKL_torch:\n",
      " 110.47222222222383 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradients of the KL divergence\n",
    "# In matlab\n",
    "C_matlab = V@torch.linalg.pinv(Sigma)\n",
    "b_matlab = torch.linalg.pinv(Sigma)@m\n",
    "\n",
    "B_matlab = dSigma@torch.linalg.pinv(Sigma) # this is written as B = dSigma(:, :, i)/Sigma; in matlab\n",
    "\n",
    "# Adding elements to the diagonal only to make this example work with non positive definite matrices\n",
    "KLD_matlab = 0.5*log_det(Sigma + 10*torch.eye(Sigma.shape[0]))  - 0.5*log_det(V+ 10*torch.eye(V.shape[0])) + 0.5*torch.sum(torch.linalg.eig(C_matlab)[0])+ 0.5*m.T@b_matlab\n",
    "\n",
    "dKL_matlab = 0.5*torch.trace(B_matlab) - 0.5*torch.trace(C_matlab@B_matlab)  - 0.5*b_matlab.T@B_matlab@m\n",
    "\n",
    "\n",
    "# In torch:\n",
    "c_torch = V @ K_tilde_inv # shape (ntilde, ntilde)\n",
    "b_torch = K_tilde_inv @ m\n",
    "\n",
    "B_torch = dK_tilde@K_tilde_inv # Shape (ntilde, ntilde\n",
    "\n",
    "KLD_torch = -0.5*(log_det(V + 10*torch.eye(V.shape[0])) - log_det(K_tilde+ 10*torch.eye(K_tilde.shape[0]))) + 0.5*torch.matmul(m.T, b_torch) + 0.5*torch.trace(c_torch) \n",
    "dKL_torch = 0.5*torch.trace(B_torch) - 0.5*torch.trace(c_torch@B_torch) - 0.5*b_torch.T@(B_torch@m)\n",
    "\n",
    "print(f' C_matlab:\\n {C_matlab} \\n c_torch:\\n {c_torch} \\n b_matlab:\\n {b_matlab} \\n b_torch:\\n {b_torch} \\n')\n",
    "print(f' B_matlab:\\n {B_matlab} \\n B_torch:\\n {B_torch} \\n')\n",
    "print(f' KLD_matlab:\\n {KLD_matlab} \\n KLD_torch:\\n {KLD_torch} \\n')\n",
    "print(f' dKL_matlab:\\n {dKL_matlab} \\n dKL_torch:\\n {dKL_torch} \\n')\n",
    "\n",
    "\n",
    "# It looks like gradients are correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
