This file explains the relatin between the hyperparameters as they where set by Matthew and Samuele in the original code, and how they are set in Pietro's.

HYPERPARAMETERS ARE SET AS:

sigma_0:  acoskern specific hyperparameter. It is sigma_0 in Samuele's notes and in the paper. 
    It's called sigmab in Samu's code, and theta[1] in matthew'a matlab code, but its initial value is set as logsigma_b 
    (sigmab gets in fact exponentiated in C, so what is learned in Samu's code is the log).
    The gradient based optimizer of the m step learns exactly sigma_0 in Pietro's code, not the log.

Amp: localker parameter. Amplitude of the localker (C). 'A' in Samuele's notes and paper. 
    It is absent in other codes, Samuele and Matthew's
    Not to be confused with the gain of the log of the firing rate.
 
 
eps_0 = (eps_0x, eps0y) : Center of the receptive field as real numbers from -1 to 1. 
    They are rescaled from the STA estimation that returns them initially as integers (pixel position)
    In Samuele's code they were set as theta[2] and [3] as real values from -1 and 1


logbetaexpr : beta is the scale of local filter of C in the paper.
    In Pietro's code logbetaexpr = -2log(2*beta), where beta is the one in the paper. 
    This expression is what is learned by the algorithm. Justification in section below.

    In Samuele's code beta was ecoded as log(beta_sam) in theta[4], as theta[2] in matthew's matlab code.
    We specify it's beta_sam because that is not beta_paper = beta_pietro, the one that appears in logbetaexpr above. 

logrhoexpr : rho is lenght scale of smoothing filter. As above, there is a discrepancy between Pietro's code and Samuele's.
    In Pietro's code logrhoexpr = -log(2rho*rho) is what is learned, where rho is the one in the paper.
    
    In Samuele's code rho was encoded as log(rho_samu) in theta[1], as theta[3] in matthew's matlab code. 
    We specify it's rho_sam because that is not rho_paper = rho_pietro, the one that appears in logrhoexpr above. 






HOW ARE HYPERPARAMETERS LEARNED BY SAMUELE'S CODE.
Reference for original Samuele's code is file /Scalable-gaussian-process-inference-of-neural-responses-to-natural-images/Variational GP-Single change-GPU-ver2.0.ipynb

theta is given in opt, inside varGP as: (line 42 of varGP cell)
    'theta': [ 0, 5, 0, 0, 5.5], # sigma_b, log(rho), eps_0_x, eps_0_y, log(beta)
    where:
             |  Is indicated as:      
    theta[0] =  sigma_b (indicated like this) but is really logsigma_b since it gets exponenxiated in line 38 of cell acoskern() (sigmab = np.exp(theta[0]))
    theta[1] =  log(rho) is correct, it only gets exponentated in localsmoothkern to make the expressions in terms of rho ( more comprehensible)
    theta[2] =  eps_0_x. Real number from -1 to 1
    theta[3] =  eps_0_y. Real number from -1 to 1
    theta[4] =  log(beta) is correct, it only gets exponentated in localsmoothkern to make the expressions in terms of rho ( more comprehensible)

    The obtained expressions for the kernel factors are:
    logf = -0.5*beta*((xcord- x0)**2+(ycord- y0)**2)
    f = np.exp(logf)    # this is supposed to be alpha local in the paper

    logK0 = -0.5*rho*((xcord - xcord[:, np.newaxis])**2+(ycord - ycord[:, np.newaxis])**2)
    K0 = np.exp(logK0)   #cij smooth in the paper

    Which is not exactly what was reported in the paper, in fact there it's:

    alpha_local = exp( -(1/2beta_paper)^2 * |eps_i-eps_0|  )       -> This means beta_samu = 1/(2*beta_paper^2)
    cij_smooth  = exp( -( 1/(2*rho_paper^2) ) ) * |eps_i-eps_j| )  -> This means  rho_samu = 1/rho_paper^2

    Learning is performed on theta, therefore parameters are learned as 'log(sigma_b)', log(rho), eps_0_x, eps_0_y, log(beta)
    
    This means that with respect to the paper parameters learning in Samu's code is done on:

    log(beta_sam) = -log(2*beta_paper^2)
    log(rho_sam)  = -log(rho_paper^2)

In Pietro's code hyperparameters are exacly defined as in the paper: beta_pietro = beta_paper and rho_pietro = rho_paper, but learning 
is still done on a logarithmic expression of them. These expressions are:

    logbetaexpr = -2log(2*beta_paper) 
    logrhoexpr  = -log(2*rho_paper^2) 

Formulas for passing from Pietro's to Samuele's code are:

    log(beta_sam) = -log(2*beta_pietro^2) = logbetaexpr + log2,        beta_sam = 1/(2*beta_pietro^2)

    log(rho_sam)  = -2log(rho_pietro)   = logrhoexpr + log2,           rho_sam  = 1/(rho_pietro^2)


Formulas for passing from the logexpressions to the paper parameters in Pietro's code are:

    beta_paper = beta_pietro = exp( -logbetaexpr/2 )/2  

    rho_paper = rho_pietro = exp( -logrhoexpr/2 )/sqrt(2)
    